{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data for sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sanity_check_data():\n",
    "\n",
    "    # generate data\n",
    "    X = np.float64(np.random.randn(120, 2))\n",
    "    Y = np.tanh(X[:, 0] + X[:, 1])\n",
    "    Y = 1. / (1. + np.exp(-(Y + Y)))\n",
    "    Y = Y > 0.5\n",
    "    Y = np.array([[1] if y==True else [0] for y in Y], dtype=np.float64)\n",
    "\n",
    "    # split it train test\n",
    "    X_train, X_test = X[0:100], X[100:120]\n",
    "    Y_train, Y_test =  Y[0:100],  Y[100:120]\n",
    "\n",
    "    # save it for loading in julia\n",
    "    np.save(f\"./data/sanity_input.npy\",X_train)\n",
    "    np.save(f\"./data/sanity_labels.npy\",Y_train)\n",
    "    np.save(f\"./data/sanity_test.npy\",X_test)\n",
    "    np.save(f\"./data/sanity_labels_test.npy\",Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sanity_check_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data for binary and multiclass clasifiation using make_blobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "def generate_train_data_using_blobs(multiclass:bool = False):\n",
    "    if multiclass:\n",
    "        no_class = 4\n",
    "    else:\n",
    "        no_class = 2\n",
    "\n",
    "    X, Y = make_blobs(n_samples=500, centers=no_class, n_features=10, random_state=0)\n",
    "    data = np.array(X)\n",
    "    labels = np.array([[y] for y in Y],dtype=np.float64)\n",
    "\n",
    "    X_train, X_test = data[0:400], data[400:500]\n",
    "    Y_train, Y_test =  labels[0:400],  labels[400:500]\n",
    "\n",
    "    # save it for loading in julia\n",
    "    np.save(f\"./data/binary_input{no_class}.npy\",X_train)\n",
    "    np.save(f\"./data/binary_labels{no_class}.npy\",Y_train)\n",
    "    np.save(f\"./data/binary_test{no_class}.npy\",X_test)\n",
    "    np.save(f\"./data/binary_labels_test{no_class}.npy\",Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for binary classification\n",
    "generate_train_data_using_blobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for multiclass classificaton\n",
    "generate_train_data_using_blobs(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset for torch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckDataset(Dataset):\n",
    "\n",
    "    def __init__(self,data_path,labels_path, multiclass:bool = False) -> None:\n",
    "        x = np.float32(np.load(data_path))\n",
    "        y = np.float32(np.load(labels_path))\n",
    "\n",
    "        if multiclass:\n",
    "            y = y.flatten()\n",
    "\n",
    "        self.x = torch.from_numpy(x)\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "        self.n_samples = y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.x,self.y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model for binary or multiclass clasification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,input_dim,interm_dim,output_dim, multiclass: bool = False):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_dim, interm_dim)\n",
    "        self.layer2 = nn.Linear(interm_dim,output_dim)\n",
    "        if multiclass:\n",
    "            self.ac = nn.Softmax()\n",
    "        else:\n",
    "            self.ac = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.ac(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(model, dataset, multiclass: bool = False):\n",
    "    x , y = dataset.get_data()\n",
    "    with torch.no_grad(): \n",
    "        outputs = model(x)\n",
    "        if multiclass:\n",
    "            predicted = [torch.argmax(lst) for lst in outputs]\n",
    "            result = [1 if y[i]==predicted[i] else 0 for i in range(len(predicted))]\n",
    "            acc = sum(result) / len(result)\n",
    "        else: \n",
    "            predicted =  [1 if x > 0.5 else 0 for x in outputs]\n",
    "            result = [1 if y[i]==predicted[i] else 0 for i in range(len(predicted))]\n",
    "            acc = sum(result) / len(result)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal training sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/20], Loss: 0.6634\n",
      "Epoch [1/10], Step [20/20], Loss: 0.8711\n",
      "Epoch [2/10], Step [10/20], Loss: 0.6774\n",
      "Epoch [2/10], Step [20/20], Loss: 0.6225\n",
      "Epoch [3/10], Step [10/20], Loss: 0.8782\n",
      "Epoch [3/10], Step [20/20], Loss: 0.7668\n",
      "Epoch [4/10], Step [10/20], Loss: 0.9296\n",
      "Epoch [4/10], Step [20/20], Loss: 0.6900\n",
      "Epoch [5/10], Step [10/20], Loss: 0.5867\n",
      "Epoch [5/10], Step [20/20], Loss: 0.5952\n",
      "Epoch [6/10], Step [10/20], Loss: 0.7423\n",
      "Epoch [6/10], Step [20/20], Loss: 0.6994\n",
      "Epoch [7/10], Step [10/20], Loss: 0.6176\n",
      "Epoch [7/10], Step [20/20], Loss: 0.7064\n",
      "Epoch [8/10], Step [10/20], Loss: 0.5604\n",
      "Epoch [8/10], Step [20/20], Loss: 0.5913\n",
      "Epoch [9/10], Step [10/20], Loss: 0.5919\n",
      "Epoch [9/10], Step [20/20], Loss: 0.6104\n",
      "Epoch [10/10], Step [10/20], Loss: 0.5286\n",
      "Epoch [10/10], Step [20/20], Loss: 0.5792\n",
      "Epoch [11/10], Step [10/20], Loss: 0.6516\n",
      "Epoch [11/10], Step [20/20], Loss: 0.4480\n",
      "Epoch [12/10], Step [10/20], Loss: 0.5679\n",
      "Epoch [12/10], Step [20/20], Loss: 0.5877\n",
      "Epoch [13/10], Step [10/20], Loss: 0.5501\n",
      "Epoch [13/10], Step [20/20], Loss: 0.7599\n",
      "Epoch [14/10], Step [10/20], Loss: 0.5552\n",
      "Epoch [14/10], Step [20/20], Loss: 0.5367\n",
      "Epoch [15/10], Step [10/20], Loss: 0.4114\n",
      "Epoch [15/10], Step [20/20], Loss: 0.5996\n",
      "Epoch [16/10], Step [10/20], Loss: 0.4926\n",
      "Epoch [16/10], Step [20/20], Loss: 0.3942\n",
      "Epoch [17/10], Step [10/20], Loss: 0.5023\n",
      "Epoch [17/10], Step [20/20], Loss: 0.7408\n",
      "Epoch [18/10], Step [10/20], Loss: 0.4027\n",
      "Epoch [18/10], Step [20/20], Loss: 0.6364\n",
      "Epoch [19/10], Step [10/20], Loss: 0.7175\n",
      "Epoch [19/10], Step [20/20], Loss: 0.5504\n",
      "Epoch [20/10], Step [10/20], Loss: 0.5059\n",
      "Epoch [20/10], Step [20/20], Loss: 0.5490\n",
      "Epoch [21/10], Step [10/20], Loss: 0.7405\n",
      "Epoch [21/10], Step [20/20], Loss: 0.4377\n",
      "Epoch [22/10], Step [10/20], Loss: 0.4514\n",
      "Epoch [22/10], Step [20/20], Loss: 0.4785\n",
      "Epoch [23/10], Step [10/20], Loss: 0.3673\n",
      "Epoch [23/10], Step [20/20], Loss: 0.3477\n",
      "Epoch [24/10], Step [10/20], Loss: 0.3540\n",
      "Epoch [24/10], Step [20/20], Loss: 0.4219\n",
      "Epoch [25/10], Step [10/20], Loss: 0.4718\n",
      "Epoch [25/10], Step [20/20], Loss: 0.4890\n",
      "Epoch [26/10], Step [10/20], Loss: 0.3637\n",
      "Epoch [26/10], Step [20/20], Loss: 0.6235\n",
      "Epoch [27/10], Step [10/20], Loss: 0.6553\n",
      "Epoch [27/10], Step [20/20], Loss: 0.4814\n",
      "Epoch [28/10], Step [10/20], Loss: 0.5765\n",
      "Epoch [28/10], Step [20/20], Loss: 0.5180\n",
      "Epoch [29/10], Step [10/20], Loss: 0.4158\n",
      "Epoch [29/10], Step [20/20], Loss: 0.3160\n",
      "Epoch [30/10], Step [10/20], Loss: 0.3830\n",
      "Epoch [30/10], Step [20/20], Loss: 0.2975\n",
      "Epoch [31/10], Step [10/20], Loss: 0.2320\n",
      "Epoch [31/10], Step [20/20], Loss: 0.3676\n",
      "Epoch [32/10], Step [10/20], Loss: 0.3483\n",
      "Epoch [32/10], Step [20/20], Loss: 0.3138\n",
      "Epoch [33/10], Step [10/20], Loss: 0.4880\n",
      "Epoch [33/10], Step [20/20], Loss: 0.4962\n",
      "Epoch [34/10], Step [10/20], Loss: 0.5198\n",
      "Epoch [34/10], Step [20/20], Loss: 0.2690\n",
      "Epoch [35/10], Step [10/20], Loss: 0.2587\n",
      "Epoch [35/10], Step [20/20], Loss: 0.2818\n",
      "Epoch [36/10], Step [10/20], Loss: 0.4811\n",
      "Epoch [36/10], Step [20/20], Loss: 0.5077\n",
      "Epoch [37/10], Step [10/20], Loss: 0.3784\n",
      "Epoch [37/10], Step [20/20], Loss: 0.2181\n",
      "Epoch [38/10], Step [10/20], Loss: 0.2907\n",
      "Epoch [38/10], Step [20/20], Loss: 0.5383\n",
      "Epoch [39/10], Step [10/20], Loss: 0.2094\n",
      "Epoch [39/10], Step [20/20], Loss: 0.4109\n",
      "Epoch [40/10], Step [10/20], Loss: 0.3544\n",
      "Epoch [40/10], Step [20/20], Loss: 0.3672\n",
      "Epoch [41/10], Step [10/20], Loss: 0.2139\n",
      "Epoch [41/10], Step [20/20], Loss: 0.2632\n",
      "Epoch [42/10], Step [10/20], Loss: 0.3024\n",
      "Epoch [42/10], Step [20/20], Loss: 0.5505\n",
      "Epoch [43/10], Step [10/20], Loss: 0.4133\n",
      "Epoch [43/10], Step [20/20], Loss: 0.2357\n",
      "Epoch [44/10], Step [10/20], Loss: 0.4514\n",
      "Epoch [44/10], Step [20/20], Loss: 0.2513\n",
      "Epoch [45/10], Step [10/20], Loss: 0.5643\n",
      "Epoch [45/10], Step [20/20], Loss: 0.2144\n",
      "Epoch [46/10], Step [10/20], Loss: 0.3760\n",
      "Epoch [46/10], Step [20/20], Loss: 0.2016\n",
      "Epoch [47/10], Step [10/20], Loss: 0.2138\n",
      "Epoch [47/10], Step [20/20], Loss: 0.3323\n",
      "Epoch [48/10], Step [10/20], Loss: 0.2287\n",
      "Epoch [48/10], Step [20/20], Loss: 0.5896\n",
      "Epoch [49/10], Step [10/20], Loss: 0.2090\n",
      "Epoch [49/10], Step [20/20], Loss: 0.2934\n",
      "Epoch [50/10], Step [10/20], Loss: 0.2202\n",
      "Epoch [50/10], Step [20/20], Loss: 0.2660\n",
      "Epoch [51/10], Step [10/20], Loss: 0.2392\n",
      "Epoch [51/10], Step [20/20], Loss: 0.2618\n",
      "Epoch [52/10], Step [10/20], Loss: 0.1732\n",
      "Epoch [52/10], Step [20/20], Loss: 0.3527\n",
      "Epoch [53/10], Step [10/20], Loss: 0.2985\n",
      "Epoch [53/10], Step [20/20], Loss: 0.1674\n",
      "Epoch [54/10], Step [10/20], Loss: 0.4326\n",
      "Epoch [54/10], Step [20/20], Loss: 0.2097\n",
      "Epoch [55/10], Step [10/20], Loss: 0.2436\n",
      "Epoch [55/10], Step [20/20], Loss: 0.1514\n",
      "Epoch [56/10], Step [10/20], Loss: 0.2028\n",
      "Epoch [56/10], Step [20/20], Loss: 0.3100\n",
      "Epoch [57/10], Step [10/20], Loss: 0.4817\n",
      "Epoch [57/10], Step [20/20], Loss: 0.1445\n",
      "Epoch [58/10], Step [10/20], Loss: 0.1853\n",
      "Epoch [58/10], Step [20/20], Loss: 0.2172\n",
      "Epoch [59/10], Step [10/20], Loss: 0.3535\n",
      "Epoch [59/10], Step [20/20], Loss: 0.1624\n",
      "Epoch [60/10], Step [10/20], Loss: 0.2475\n",
      "Epoch [60/10], Step [20/20], Loss: 0.3188\n",
      "Epoch [61/10], Step [10/20], Loss: 0.1315\n",
      "Epoch [61/10], Step [20/20], Loss: 0.3095\n",
      "Epoch [62/10], Step [10/20], Loss: 0.1210\n",
      "Epoch [62/10], Step [20/20], Loss: 0.3862\n",
      "Epoch [63/10], Step [10/20], Loss: 0.1583\n",
      "Epoch [63/10], Step [20/20], Loss: 0.2116\n",
      "Epoch [64/10], Step [10/20], Loss: 0.1004\n",
      "Epoch [64/10], Step [20/20], Loss: 0.2038\n",
      "Epoch [65/10], Step [10/20], Loss: 0.1244\n",
      "Epoch [65/10], Step [20/20], Loss: 0.1754\n",
      "Epoch [66/10], Step [10/20], Loss: 0.1777\n",
      "Epoch [66/10], Step [20/20], Loss: 0.1274\n",
      "Epoch [67/10], Step [10/20], Loss: 0.1056\n",
      "Epoch [67/10], Step [20/20], Loss: 0.1921\n",
      "Epoch [68/10], Step [10/20], Loss: 0.3175\n",
      "Epoch [68/10], Step [20/20], Loss: 0.1486\n",
      "Epoch [69/10], Step [10/20], Loss: 0.3369\n",
      "Epoch [69/10], Step [20/20], Loss: 0.1430\n",
      "Epoch [70/10], Step [10/20], Loss: 0.0960\n",
      "Epoch [70/10], Step [20/20], Loss: 0.1620\n",
      "Epoch [71/10], Step [10/20], Loss: 0.2559\n",
      "Epoch [71/10], Step [20/20], Loss: 0.2600\n",
      "Epoch [72/10], Step [10/20], Loss: 0.3255\n",
      "Epoch [72/10], Step [20/20], Loss: 0.1544\n",
      "Epoch [73/10], Step [10/20], Loss: 0.3131\n",
      "Epoch [73/10], Step [20/20], Loss: 0.1376\n",
      "Epoch [74/10], Step [10/20], Loss: 0.2204\n",
      "Epoch [74/10], Step [20/20], Loss: 0.1344\n",
      "Epoch [75/10], Step [10/20], Loss: 0.0782\n",
      "Epoch [75/10], Step [20/20], Loss: 0.0999\n",
      "Epoch [76/10], Step [10/20], Loss: 0.2093\n",
      "Epoch [76/10], Step [20/20], Loss: 0.0910\n",
      "Epoch [77/10], Step [10/20], Loss: 0.1590\n",
      "Epoch [77/10], Step [20/20], Loss: 0.2077\n",
      "Epoch [78/10], Step [10/20], Loss: 0.2287\n",
      "Epoch [78/10], Step [20/20], Loss: 0.2049\n",
      "Epoch [79/10], Step [10/20], Loss: 0.1350\n",
      "Epoch [79/10], Step [20/20], Loss: 0.1304\n",
      "Epoch [80/10], Step [10/20], Loss: 0.1847\n",
      "Epoch [80/10], Step [20/20], Loss: 0.1330\n",
      "Epoch [81/10], Step [10/20], Loss: 0.1961\n",
      "Epoch [81/10], Step [20/20], Loss: 0.1723\n",
      "Epoch [82/10], Step [10/20], Loss: 0.2774\n",
      "Epoch [82/10], Step [20/20], Loss: 0.1580\n",
      "Epoch [83/10], Step [10/20], Loss: 0.1453\n",
      "Epoch [83/10], Step [20/20], Loss: 0.0632\n",
      "Epoch [84/10], Step [10/20], Loss: 0.0515\n",
      "Epoch [84/10], Step [20/20], Loss: 0.1311\n",
      "Epoch [85/10], Step [10/20], Loss: 0.0548\n",
      "Epoch [85/10], Step [20/20], Loss: 0.0637\n",
      "Epoch [86/10], Step [10/20], Loss: 0.1315\n",
      "Epoch [86/10], Step [20/20], Loss: 0.1206\n",
      "Epoch [87/10], Step [10/20], Loss: 0.0713\n",
      "Epoch [87/10], Step [20/20], Loss: 0.0671\n",
      "Epoch [88/10], Step [10/20], Loss: 0.1351\n",
      "Epoch [88/10], Step [20/20], Loss: 0.1481\n",
      "Epoch [89/10], Step [10/20], Loss: 0.0707\n",
      "Epoch [89/10], Step [20/20], Loss: 0.1186\n",
      "Epoch [90/10], Step [10/20], Loss: 0.2574\n",
      "Epoch [90/10], Step [20/20], Loss: 0.2745\n",
      "Epoch [91/10], Step [10/20], Loss: 0.0465\n",
      "Epoch [91/10], Step [20/20], Loss: 0.1932\n",
      "Epoch [92/10], Step [10/20], Loss: 0.1251\n",
      "Epoch [92/10], Step [20/20], Loss: 0.1374\n",
      "Epoch [93/10], Step [10/20], Loss: 0.1968\n",
      "Epoch [93/10], Step [20/20], Loss: 0.1264\n",
      "Epoch [94/10], Step [10/20], Loss: 0.0822\n",
      "Epoch [94/10], Step [20/20], Loss: 0.0529\n",
      "Epoch [95/10], Step [10/20], Loss: 0.1514\n",
      "Epoch [95/10], Step [20/20], Loss: 0.0742\n",
      "Epoch [96/10], Step [10/20], Loss: 0.1522\n",
      "Epoch [96/10], Step [20/20], Loss: 0.1141\n",
      "Epoch [97/10], Step [10/20], Loss: 0.1954\n",
      "Epoch [97/10], Step [20/20], Loss: 0.1088\n",
      "Epoch [98/10], Step [10/20], Loss: 0.0599\n",
      "Epoch [98/10], Step [20/20], Loss: 0.1046\n",
      "Epoch [99/10], Step [10/20], Loss: 0.0898\n",
      "Epoch [99/10], Step [20/20], Loss: 0.1849\n",
      "Epoch [100/10], Step [10/20], Loss: 0.1255\n",
      "Epoch [100/10], Step [20/20], Loss: 0.0413\n",
      "Finish training\n",
      "Accuracy on training dataset: 1.0\n",
      "Accuracy on testing dataset: 0.98\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sanity_check_model = ClassificationModel(2,2,1).to(device)\n",
    "sanity_check_train_dataset = CheckDataset(data_path = \"./data/sanity_input.npy\",\n",
    "                                                labels_path= \"./data/sanity_labels.npy\")\n",
    "saniry_check_test_dataset = CheckDataset(data_path = \"./data/sanity_test.npy\",\n",
    "                                               labels_path= \"./data/sanity_labels_test.npy\")\n",
    "\n",
    "sanity_check_train_loader = DataLoader(dataset=sanity_check_train_dataset, batch_size=5, shuffle=True)\n",
    "sanity_check_test_loader = DataLoader(dataset=saniry_check_test_dataset, batch_size=5, shuffle=False)\n",
    "optimizer = torch.optim.SGD(sanity_check_model.parameters(), lr=0.01)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "n_total_steps = len(sanity_check_train_loader)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, (inputs, labels) in enumerate(sanity_check_train_loader):\n",
    "\n",
    "        # forward pass and loss\n",
    "        outputs = sanity_check_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # compute gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Finish training\")\n",
    "\n",
    "print(\"Accuracy on training dataset: {}\".format(get_acc(sanity_check_model, saniry_check_test_dataset)))\n",
    "print(\"Accuracy on testing dataset: {}\".format(get_acc(sanity_check_model, sanity_check_train_dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal training binary classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/40], Loss: 0.6815\n",
      "Epoch [1/10], Step [20/40], Loss: 0.5005\n",
      "Epoch [1/10], Step [30/40], Loss: 0.4707\n",
      "Epoch [1/10], Step [40/40], Loss: 0.4676\n",
      "Epoch [2/10], Step [10/40], Loss: 0.3013\n",
      "Epoch [2/10], Step [20/40], Loss: 0.4104\n",
      "Epoch [2/10], Step [30/40], Loss: 0.3389\n",
      "Epoch [2/10], Step [40/40], Loss: 0.3684\n",
      "Epoch [3/10], Step [10/40], Loss: 0.2889\n",
      "Epoch [3/10], Step [20/40], Loss: 0.2762\n",
      "Epoch [3/10], Step [30/40], Loss: 0.2814\n",
      "Epoch [3/10], Step [40/40], Loss: 0.2562\n",
      "Epoch [4/10], Step [10/40], Loss: 0.2880\n",
      "Epoch [4/10], Step [20/40], Loss: 0.2247\n",
      "Epoch [4/10], Step [30/40], Loss: 0.2145\n",
      "Epoch [4/10], Step [40/40], Loss: 0.2635\n",
      "Epoch [5/10], Step [10/40], Loss: 0.2131\n",
      "Epoch [5/10], Step [20/40], Loss: 0.1925\n",
      "Epoch [5/10], Step [30/40], Loss: 0.2147\n",
      "Epoch [5/10], Step [40/40], Loss: 0.2258\n",
      "Epoch [6/10], Step [10/40], Loss: 0.2081\n",
      "Epoch [6/10], Step [20/40], Loss: 0.2095\n",
      "Epoch [6/10], Step [30/40], Loss: 0.1645\n",
      "Epoch [6/10], Step [40/40], Loss: 0.1757\n",
      "Epoch [7/10], Step [10/40], Loss: 0.1699\n",
      "Epoch [7/10], Step [20/40], Loss: 0.1714\n",
      "Epoch [7/10], Step [30/40], Loss: 0.1609\n",
      "Epoch [7/10], Step [40/40], Loss: 0.1791\n",
      "Epoch [8/10], Step [10/40], Loss: 0.1582\n",
      "Epoch [8/10], Step [20/40], Loss: 0.1648\n",
      "Epoch [8/10], Step [30/40], Loss: 0.1407\n",
      "Epoch [8/10], Step [40/40], Loss: 0.1548\n",
      "Epoch [9/10], Step [10/40], Loss: 0.1469\n",
      "Epoch [9/10], Step [20/40], Loss: 0.1424\n",
      "Epoch [9/10], Step [30/40], Loss: 0.1314\n",
      "Epoch [9/10], Step [40/40], Loss: 0.1357\n",
      "Epoch [10/10], Step [10/40], Loss: 0.1283\n",
      "Epoch [10/10], Step [20/40], Loss: 0.1332\n",
      "Epoch [10/10], Step [30/40], Loss: 0.1195\n",
      "Epoch [10/10], Step [40/40], Loss: 0.1253\n",
      "Epoch [11/10], Step [10/40], Loss: 0.1208\n",
      "Epoch [11/10], Step [20/40], Loss: 0.1159\n",
      "Epoch [11/10], Step [30/40], Loss: 0.1170\n",
      "Epoch [11/10], Step [40/40], Loss: 0.1175\n",
      "Epoch [12/10], Step [10/40], Loss: 0.1056\n",
      "Epoch [12/10], Step [20/40], Loss: 0.1124\n",
      "Epoch [12/10], Step [30/40], Loss: 0.1017\n",
      "Epoch [12/10], Step [40/40], Loss: 0.0986\n",
      "Epoch [13/10], Step [10/40], Loss: 0.1013\n",
      "Epoch [13/10], Step [20/40], Loss: 0.1017\n",
      "Epoch [13/10], Step [30/40], Loss: 0.1016\n",
      "Epoch [13/10], Step [40/40], Loss: 0.0929\n",
      "Epoch [14/10], Step [10/40], Loss: 0.0939\n",
      "Epoch [14/10], Step [20/40], Loss: 0.0898\n",
      "Epoch [14/10], Step [30/40], Loss: 0.0881\n",
      "Epoch [14/10], Step [40/40], Loss: 0.0854\n",
      "Epoch [15/10], Step [10/40], Loss: 0.0918\n",
      "Epoch [15/10], Step [20/40], Loss: 0.0922\n",
      "Epoch [15/10], Step [30/40], Loss: 0.0866\n",
      "Epoch [15/10], Step [40/40], Loss: 0.0855\n",
      "Epoch [16/10], Step [10/40], Loss: 0.0787\n",
      "Epoch [16/10], Step [20/40], Loss: 0.0780\n",
      "Epoch [16/10], Step [30/40], Loss: 0.0780\n",
      "Epoch [16/10], Step [40/40], Loss: 0.0769\n",
      "Epoch [17/10], Step [10/40], Loss: 0.0728\n",
      "Epoch [17/10], Step [20/40], Loss: 0.0762\n",
      "Epoch [17/10], Step [30/40], Loss: 0.0755\n",
      "Epoch [17/10], Step [40/40], Loss: 0.0743\n",
      "Epoch [18/10], Step [10/40], Loss: 0.0716\n",
      "Epoch [18/10], Step [20/40], Loss: 0.0665\n",
      "Epoch [18/10], Step [30/40], Loss: 0.0719\n",
      "Epoch [18/10], Step [40/40], Loss: 0.0690\n",
      "Epoch [19/10], Step [10/40], Loss: 0.0670\n",
      "Epoch [19/10], Step [20/40], Loss: 0.0689\n",
      "Epoch [19/10], Step [30/40], Loss: 0.0697\n",
      "Epoch [19/10], Step [40/40], Loss: 0.0695\n",
      "Epoch [20/10], Step [10/40], Loss: 0.0661\n",
      "Epoch [20/10], Step [20/40], Loss: 0.0640\n",
      "Epoch [20/10], Step [30/40], Loss: 0.0657\n",
      "Epoch [20/10], Step [40/40], Loss: 0.0635\n",
      "Epoch [21/10], Step [10/40], Loss: 0.0628\n",
      "Epoch [21/10], Step [20/40], Loss: 0.0609\n",
      "Epoch [21/10], Step [30/40], Loss: 0.0602\n",
      "Epoch [21/10], Step [40/40], Loss: 0.0639\n",
      "Epoch [22/10], Step [10/40], Loss: 0.0591\n",
      "Epoch [22/10], Step [20/40], Loss: 0.0583\n",
      "Epoch [22/10], Step [30/40], Loss: 0.0597\n",
      "Epoch [22/10], Step [40/40], Loss: 0.0571\n",
      "Epoch [23/10], Step [10/40], Loss: 0.0585\n",
      "Epoch [23/10], Step [20/40], Loss: 0.0567\n",
      "Epoch [23/10], Step [30/40], Loss: 0.0592\n",
      "Epoch [23/10], Step [40/40], Loss: 0.0528\n",
      "Epoch [24/10], Step [10/40], Loss: 0.0559\n",
      "Epoch [24/10], Step [20/40], Loss: 0.0527\n",
      "Epoch [24/10], Step [30/40], Loss: 0.0531\n",
      "Epoch [24/10], Step [40/40], Loss: 0.0543\n",
      "Epoch [25/10], Step [10/40], Loss: 0.0552\n",
      "Epoch [25/10], Step [20/40], Loss: 0.0547\n",
      "Epoch [25/10], Step [30/40], Loss: 0.0494\n",
      "Epoch [25/10], Step [40/40], Loss: 0.0496\n",
      "Epoch [26/10], Step [10/40], Loss: 0.0508\n",
      "Epoch [26/10], Step [20/40], Loss: 0.0479\n",
      "Epoch [26/10], Step [30/40], Loss: 0.0499\n",
      "Epoch [26/10], Step [40/40], Loss: 0.0494\n",
      "Epoch [27/10], Step [10/40], Loss: 0.0467\n",
      "Epoch [27/10], Step [20/40], Loss: 0.0492\n",
      "Epoch [27/10], Step [30/40], Loss: 0.0472\n",
      "Epoch [27/10], Step [40/40], Loss: 0.0476\n",
      "Epoch [28/10], Step [10/40], Loss: 0.0444\n",
      "Epoch [28/10], Step [20/40], Loss: 0.0460\n",
      "Epoch [28/10], Step [30/40], Loss: 0.0449\n",
      "Epoch [28/10], Step [40/40], Loss: 0.0451\n",
      "Epoch [29/10], Step [10/40], Loss: 0.0454\n",
      "Epoch [29/10], Step [20/40], Loss: 0.0457\n",
      "Epoch [29/10], Step [30/40], Loss: 0.0457\n",
      "Epoch [29/10], Step [40/40], Loss: 0.0444\n",
      "Epoch [30/10], Step [10/40], Loss: 0.0451\n",
      "Epoch [30/10], Step [20/40], Loss: 0.0412\n",
      "Epoch [30/10], Step [30/40], Loss: 0.0422\n",
      "Epoch [30/10], Step [40/40], Loss: 0.0443\n",
      "Epoch [31/10], Step [10/40], Loss: 0.0426\n",
      "Epoch [31/10], Step [20/40], Loss: 0.0440\n",
      "Epoch [31/10], Step [30/40], Loss: 0.0424\n",
      "Epoch [31/10], Step [40/40], Loss: 0.0409\n",
      "Epoch [32/10], Step [10/40], Loss: 0.0400\n",
      "Epoch [32/10], Step [20/40], Loss: 0.0427\n",
      "Epoch [32/10], Step [30/40], Loss: 0.0400\n",
      "Epoch [32/10], Step [40/40], Loss: 0.0412\n",
      "Epoch [33/10], Step [10/40], Loss: 0.0410\n",
      "Epoch [33/10], Step [20/40], Loss: 0.0398\n",
      "Epoch [33/10], Step [30/40], Loss: 0.0399\n",
      "Epoch [33/10], Step [40/40], Loss: 0.0395\n",
      "Epoch [34/10], Step [10/40], Loss: 0.0382\n",
      "Epoch [34/10], Step [20/40], Loss: 0.0394\n",
      "Epoch [34/10], Step [30/40], Loss: 0.0390\n",
      "Epoch [34/10], Step [40/40], Loss: 0.0392\n",
      "Epoch [35/10], Step [10/40], Loss: 0.0374\n",
      "Epoch [35/10], Step [20/40], Loss: 0.0366\n",
      "Epoch [35/10], Step [30/40], Loss: 0.0376\n",
      "Epoch [35/10], Step [40/40], Loss: 0.0364\n",
      "Epoch [36/10], Step [10/40], Loss: 0.0380\n",
      "Epoch [36/10], Step [20/40], Loss: 0.0364\n",
      "Epoch [36/10], Step [30/40], Loss: 0.0369\n",
      "Epoch [36/10], Step [40/40], Loss: 0.0361\n",
      "Epoch [37/10], Step [10/40], Loss: 0.0368\n",
      "Epoch [37/10], Step [20/40], Loss: 0.0336\n",
      "Epoch [37/10], Step [30/40], Loss: 0.0354\n",
      "Epoch [37/10], Step [40/40], Loss: 0.0357\n",
      "Epoch [38/10], Step [10/40], Loss: 0.0351\n",
      "Epoch [38/10], Step [20/40], Loss: 0.0352\n",
      "Epoch [38/10], Step [30/40], Loss: 0.0337\n",
      "Epoch [38/10], Step [40/40], Loss: 0.0340\n",
      "Epoch [39/10], Step [10/40], Loss: 0.0336\n",
      "Epoch [39/10], Step [20/40], Loss: 0.0331\n",
      "Epoch [39/10], Step [30/40], Loss: 0.0332\n",
      "Epoch [39/10], Step [40/40], Loss: 0.0326\n",
      "Epoch [40/10], Step [10/40], Loss: 0.0324\n",
      "Epoch [40/10], Step [20/40], Loss: 0.0315\n",
      "Epoch [40/10], Step [30/40], Loss: 0.0338\n",
      "Epoch [40/10], Step [40/40], Loss: 0.0316\n",
      "Epoch [41/10], Step [10/40], Loss: 0.0321\n",
      "Epoch [41/10], Step [20/40], Loss: 0.0316\n",
      "Epoch [41/10], Step [30/40], Loss: 0.0327\n",
      "Epoch [41/10], Step [40/40], Loss: 0.0314\n",
      "Epoch [42/10], Step [10/40], Loss: 0.0319\n",
      "Epoch [42/10], Step [20/40], Loss: 0.0307\n",
      "Epoch [42/10], Step [30/40], Loss: 0.0305\n",
      "Epoch [42/10], Step [40/40], Loss: 0.0310\n",
      "Epoch [43/10], Step [10/40], Loss: 0.0309\n",
      "Epoch [43/10], Step [20/40], Loss: 0.0293\n",
      "Epoch [43/10], Step [30/40], Loss: 0.0307\n",
      "Epoch [43/10], Step [40/40], Loss: 0.0310\n",
      "Epoch [44/10], Step [10/40], Loss: 0.0294\n",
      "Epoch [44/10], Step [20/40], Loss: 0.0300\n",
      "Epoch [44/10], Step [30/40], Loss: 0.0292\n",
      "Epoch [44/10], Step [40/40], Loss: 0.0287\n",
      "Epoch [45/10], Step [10/40], Loss: 0.0293\n",
      "Epoch [45/10], Step [20/40], Loss: 0.0292\n",
      "Epoch [45/10], Step [30/40], Loss: 0.0291\n",
      "Epoch [45/10], Step [40/40], Loss: 0.0286\n",
      "Epoch [46/10], Step [10/40], Loss: 0.0285\n",
      "Epoch [46/10], Step [20/40], Loss: 0.0278\n",
      "Epoch [46/10], Step [30/40], Loss: 0.0288\n",
      "Epoch [46/10], Step [40/40], Loss: 0.0287\n",
      "Epoch [47/10], Step [10/40], Loss: 0.0276\n",
      "Epoch [47/10], Step [20/40], Loss: 0.0283\n",
      "Epoch [47/10], Step [30/40], Loss: 0.0277\n",
      "Epoch [47/10], Step [40/40], Loss: 0.0280\n",
      "Epoch [48/10], Step [10/40], Loss: 0.0272\n",
      "Epoch [48/10], Step [20/40], Loss: 0.0266\n",
      "Epoch [48/10], Step [30/40], Loss: 0.0278\n",
      "Epoch [48/10], Step [40/40], Loss: 0.0268\n",
      "Epoch [49/10], Step [10/40], Loss: 0.0269\n",
      "Epoch [49/10], Step [20/40], Loss: 0.0263\n",
      "Epoch [49/10], Step [30/40], Loss: 0.0261\n",
      "Epoch [49/10], Step [40/40], Loss: 0.0262\n",
      "Epoch [50/10], Step [10/40], Loss: 0.0261\n",
      "Epoch [50/10], Step [20/40], Loss: 0.0261\n",
      "Epoch [50/10], Step [30/40], Loss: 0.0253\n",
      "Epoch [50/10], Step [40/40], Loss: 0.0255\n",
      "Epoch [51/10], Step [10/40], Loss: 0.0261\n",
      "Epoch [51/10], Step [20/40], Loss: 0.0257\n",
      "Epoch [51/10], Step [30/40], Loss: 0.0253\n",
      "Epoch [51/10], Step [40/40], Loss: 0.0250\n",
      "Epoch [52/10], Step [10/40], Loss: 0.0258\n",
      "Epoch [52/10], Step [20/40], Loss: 0.0253\n",
      "Epoch [52/10], Step [30/40], Loss: 0.0251\n",
      "Epoch [52/10], Step [40/40], Loss: 0.0247\n",
      "Epoch [53/10], Step [10/40], Loss: 0.0241\n",
      "Epoch [53/10], Step [20/40], Loss: 0.0247\n",
      "Epoch [53/10], Step [30/40], Loss: 0.0248\n",
      "Epoch [53/10], Step [40/40], Loss: 0.0245\n",
      "Epoch [54/10], Step [10/40], Loss: 0.0241\n",
      "Epoch [54/10], Step [20/40], Loss: 0.0236\n",
      "Epoch [54/10], Step [30/40], Loss: 0.0237\n",
      "Epoch [54/10], Step [40/40], Loss: 0.0234\n",
      "Epoch [55/10], Step [10/40], Loss: 0.0232\n",
      "Epoch [55/10], Step [20/40], Loss: 0.0247\n",
      "Epoch [55/10], Step [30/40], Loss: 0.0235\n",
      "Epoch [55/10], Step [40/40], Loss: 0.0232\n",
      "Epoch [56/10], Step [10/40], Loss: 0.0235\n",
      "Epoch [56/10], Step [20/40], Loss: 0.0235\n",
      "Epoch [56/10], Step [30/40], Loss: 0.0233\n",
      "Epoch [56/10], Step [40/40], Loss: 0.0238\n",
      "Epoch [57/10], Step [10/40], Loss: 0.0227\n",
      "Epoch [57/10], Step [20/40], Loss: 0.0232\n",
      "Epoch [57/10], Step [30/40], Loss: 0.0230\n",
      "Epoch [57/10], Step [40/40], Loss: 0.0219\n",
      "Epoch [58/10], Step [10/40], Loss: 0.0229\n",
      "Epoch [58/10], Step [20/40], Loss: 0.0217\n",
      "Epoch [58/10], Step [30/40], Loss: 0.0227\n",
      "Epoch [58/10], Step [40/40], Loss: 0.0224\n",
      "Epoch [59/10], Step [10/40], Loss: 0.0219\n",
      "Epoch [59/10], Step [20/40], Loss: 0.0230\n",
      "Epoch [59/10], Step [30/40], Loss: 0.0219\n",
      "Epoch [59/10], Step [40/40], Loss: 0.0210\n",
      "Epoch [60/10], Step [10/40], Loss: 0.0213\n",
      "Epoch [60/10], Step [20/40], Loss: 0.0224\n",
      "Epoch [60/10], Step [30/40], Loss: 0.0208\n",
      "Epoch [60/10], Step [40/40], Loss: 0.0214\n",
      "Epoch [61/10], Step [10/40], Loss: 0.0210\n",
      "Epoch [61/10], Step [20/40], Loss: 0.0212\n",
      "Epoch [61/10], Step [30/40], Loss: 0.0211\n",
      "Epoch [61/10], Step [40/40], Loss: 0.0209\n",
      "Epoch [62/10], Step [10/40], Loss: 0.0212\n",
      "Epoch [62/10], Step [20/40], Loss: 0.0209\n",
      "Epoch [62/10], Step [30/40], Loss: 0.0215\n",
      "Epoch [62/10], Step [40/40], Loss: 0.0205\n",
      "Epoch [63/10], Step [10/40], Loss: 0.0206\n",
      "Epoch [63/10], Step [20/40], Loss: 0.0204\n",
      "Epoch [63/10], Step [30/40], Loss: 0.0209\n",
      "Epoch [63/10], Step [40/40], Loss: 0.0204\n",
      "Epoch [64/10], Step [10/40], Loss: 0.0206\n",
      "Epoch [64/10], Step [20/40], Loss: 0.0201\n",
      "Epoch [64/10], Step [30/40], Loss: 0.0204\n",
      "Epoch [64/10], Step [40/40], Loss: 0.0202\n",
      "Epoch [65/10], Step [10/40], Loss: 0.0198\n",
      "Epoch [65/10], Step [20/40], Loss: 0.0201\n",
      "Epoch [65/10], Step [30/40], Loss: 0.0200\n",
      "Epoch [65/10], Step [40/40], Loss: 0.0201\n",
      "Epoch [66/10], Step [10/40], Loss: 0.0198\n",
      "Epoch [66/10], Step [20/40], Loss: 0.0198\n",
      "Epoch [66/10], Step [30/40], Loss: 0.0195\n",
      "Epoch [66/10], Step [40/40], Loss: 0.0194\n",
      "Epoch [67/10], Step [10/40], Loss: 0.0194\n",
      "Epoch [67/10], Step [20/40], Loss: 0.0196\n",
      "Epoch [67/10], Step [30/40], Loss: 0.0194\n",
      "Epoch [67/10], Step [40/40], Loss: 0.0194\n",
      "Epoch [68/10], Step [10/40], Loss: 0.0188\n",
      "Epoch [68/10], Step [20/40], Loss: 0.0191\n",
      "Epoch [68/10], Step [30/40], Loss: 0.0187\n",
      "Epoch [68/10], Step [40/40], Loss: 0.0188\n",
      "Epoch [69/10], Step [10/40], Loss: 0.0188\n",
      "Epoch [69/10], Step [20/40], Loss: 0.0185\n",
      "Epoch [69/10], Step [30/40], Loss: 0.0186\n",
      "Epoch [69/10], Step [40/40], Loss: 0.0186\n",
      "Epoch [70/10], Step [10/40], Loss: 0.0183\n",
      "Epoch [70/10], Step [20/40], Loss: 0.0185\n",
      "Epoch [70/10], Step [30/40], Loss: 0.0186\n",
      "Epoch [70/10], Step [40/40], Loss: 0.0186\n",
      "Epoch [71/10], Step [10/40], Loss: 0.0188\n",
      "Epoch [71/10], Step [20/40], Loss: 0.0185\n",
      "Epoch [71/10], Step [30/40], Loss: 0.0183\n",
      "Epoch [71/10], Step [40/40], Loss: 0.0180\n",
      "Epoch [72/10], Step [10/40], Loss: 0.0177\n",
      "Epoch [72/10], Step [20/40], Loss: 0.0178\n",
      "Epoch [72/10], Step [30/40], Loss: 0.0182\n",
      "Epoch [72/10], Step [40/40], Loss: 0.0175\n",
      "Epoch [73/10], Step [10/40], Loss: 0.0181\n",
      "Epoch [73/10], Step [20/40], Loss: 0.0175\n",
      "Epoch [73/10], Step [30/40], Loss: 0.0179\n",
      "Epoch [73/10], Step [40/40], Loss: 0.0176\n",
      "Epoch [74/10], Step [10/40], Loss: 0.0176\n",
      "Epoch [74/10], Step [20/40], Loss: 0.0176\n",
      "Epoch [74/10], Step [30/40], Loss: 0.0174\n",
      "Epoch [74/10], Step [40/40], Loss: 0.0169\n",
      "Epoch [75/10], Step [10/40], Loss: 0.0172\n",
      "Epoch [75/10], Step [20/40], Loss: 0.0173\n",
      "Epoch [75/10], Step [30/40], Loss: 0.0171\n",
      "Epoch [75/10], Step [40/40], Loss: 0.0165\n",
      "Epoch [76/10], Step [10/40], Loss: 0.0170\n",
      "Epoch [76/10], Step [20/40], Loss: 0.0170\n",
      "Epoch [76/10], Step [30/40], Loss: 0.0169\n",
      "Epoch [76/10], Step [40/40], Loss: 0.0170\n",
      "Epoch [77/10], Step [10/40], Loss: 0.0168\n",
      "Epoch [77/10], Step [20/40], Loss: 0.0169\n",
      "Epoch [77/10], Step [30/40], Loss: 0.0166\n",
      "Epoch [77/10], Step [40/40], Loss: 0.0164\n",
      "Epoch [78/10], Step [10/40], Loss: 0.0168\n",
      "Epoch [78/10], Step [20/40], Loss: 0.0164\n",
      "Epoch [78/10], Step [30/40], Loss: 0.0167\n",
      "Epoch [78/10], Step [40/40], Loss: 0.0163\n",
      "Epoch [79/10], Step [10/40], Loss: 0.0164\n",
      "Epoch [79/10], Step [20/40], Loss: 0.0165\n",
      "Epoch [79/10], Step [30/40], Loss: 0.0161\n",
      "Epoch [79/10], Step [40/40], Loss: 0.0160\n",
      "Epoch [80/10], Step [10/40], Loss: 0.0164\n",
      "Epoch [80/10], Step [20/40], Loss: 0.0159\n",
      "Epoch [80/10], Step [30/40], Loss: 0.0160\n",
      "Epoch [80/10], Step [40/40], Loss: 0.0159\n",
      "Epoch [81/10], Step [10/40], Loss: 0.0158\n",
      "Epoch [81/10], Step [20/40], Loss: 0.0156\n",
      "Epoch [81/10], Step [30/40], Loss: 0.0155\n",
      "Epoch [81/10], Step [40/40], Loss: 0.0162\n",
      "Epoch [82/10], Step [10/40], Loss: 0.0159\n",
      "Epoch [82/10], Step [20/40], Loss: 0.0157\n",
      "Epoch [82/10], Step [30/40], Loss: 0.0160\n",
      "Epoch [82/10], Step [40/40], Loss: 0.0156\n",
      "Epoch [83/10], Step [10/40], Loss: 0.0160\n",
      "Epoch [83/10], Step [20/40], Loss: 0.0153\n",
      "Epoch [83/10], Step [30/40], Loss: 0.0153\n",
      "Epoch [83/10], Step [40/40], Loss: 0.0154\n",
      "Epoch [84/10], Step [10/40], Loss: 0.0156\n",
      "Epoch [84/10], Step [20/40], Loss: 0.0154\n",
      "Epoch [84/10], Step [30/40], Loss: 0.0152\n",
      "Epoch [84/10], Step [40/40], Loss: 0.0153\n",
      "Epoch [85/10], Step [10/40], Loss: 0.0152\n",
      "Epoch [85/10], Step [20/40], Loss: 0.0150\n",
      "Epoch [85/10], Step [30/40], Loss: 0.0149\n",
      "Epoch [85/10], Step [40/40], Loss: 0.0150\n",
      "Epoch [86/10], Step [10/40], Loss: 0.0153\n",
      "Epoch [86/10], Step [20/40], Loss: 0.0150\n",
      "Epoch [86/10], Step [30/40], Loss: 0.0146\n",
      "Epoch [86/10], Step [40/40], Loss: 0.0149\n",
      "Epoch [87/10], Step [10/40], Loss: 0.0146\n",
      "Epoch [87/10], Step [20/40], Loss: 0.0148\n",
      "Epoch [87/10], Step [30/40], Loss: 0.0146\n",
      "Epoch [87/10], Step [40/40], Loss: 0.0148\n",
      "Epoch [88/10], Step [10/40], Loss: 0.0147\n",
      "Epoch [88/10], Step [20/40], Loss: 0.0149\n",
      "Epoch [88/10], Step [30/40], Loss: 0.0145\n",
      "Epoch [88/10], Step [40/40], Loss: 0.0145\n",
      "Epoch [89/10], Step [10/40], Loss: 0.0143\n",
      "Epoch [89/10], Step [20/40], Loss: 0.0144\n",
      "Epoch [89/10], Step [30/40], Loss: 0.0145\n",
      "Epoch [89/10], Step [40/40], Loss: 0.0145\n",
      "Epoch [90/10], Step [10/40], Loss: 0.0144\n",
      "Epoch [90/10], Step [20/40], Loss: 0.0142\n",
      "Epoch [90/10], Step [30/40], Loss: 0.0144\n",
      "Epoch [90/10], Step [40/40], Loss: 0.0144\n",
      "Epoch [91/10], Step [10/40], Loss: 0.0144\n",
      "Epoch [91/10], Step [20/40], Loss: 0.0142\n",
      "Epoch [91/10], Step [30/40], Loss: 0.0143\n",
      "Epoch [91/10], Step [40/40], Loss: 0.0143\n",
      "Epoch [92/10], Step [10/40], Loss: 0.0138\n",
      "Epoch [92/10], Step [20/40], Loss: 0.0139\n",
      "Epoch [92/10], Step [30/40], Loss: 0.0138\n",
      "Epoch [92/10], Step [40/40], Loss: 0.0142\n",
      "Epoch [93/10], Step [10/40], Loss: 0.0138\n",
      "Epoch [93/10], Step [20/40], Loss: 0.0137\n",
      "Epoch [93/10], Step [30/40], Loss: 0.0138\n",
      "Epoch [93/10], Step [40/40], Loss: 0.0138\n",
      "Epoch [94/10], Step [10/40], Loss: 0.0139\n",
      "Epoch [94/10], Step [20/40], Loss: 0.0138\n",
      "Epoch [94/10], Step [30/40], Loss: 0.0135\n",
      "Epoch [94/10], Step [40/40], Loss: 0.0137\n",
      "Epoch [95/10], Step [10/40], Loss: 0.0139\n",
      "Epoch [95/10], Step [20/40], Loss: 0.0135\n",
      "Epoch [95/10], Step [30/40], Loss: 0.0135\n",
      "Epoch [95/10], Step [40/40], Loss: 0.0137\n",
      "Epoch [96/10], Step [10/40], Loss: 0.0135\n",
      "Epoch [96/10], Step [20/40], Loss: 0.0132\n",
      "Epoch [96/10], Step [30/40], Loss: 0.0132\n",
      "Epoch [96/10], Step [40/40], Loss: 0.0136\n",
      "Epoch [97/10], Step [10/40], Loss: 0.0131\n",
      "Epoch [97/10], Step [20/40], Loss: 0.0132\n",
      "Epoch [97/10], Step [30/40], Loss: 0.0129\n",
      "Epoch [97/10], Step [40/40], Loss: 0.0134\n",
      "Epoch [98/10], Step [10/40], Loss: 0.0131\n",
      "Epoch [98/10], Step [20/40], Loss: 0.0132\n",
      "Epoch [98/10], Step [30/40], Loss: 0.0129\n",
      "Epoch [98/10], Step [40/40], Loss: 0.0130\n",
      "Epoch [99/10], Step [10/40], Loss: 0.0131\n",
      "Epoch [99/10], Step [20/40], Loss: 0.0131\n",
      "Epoch [99/10], Step [30/40], Loss: 0.0132\n",
      "Epoch [99/10], Step [40/40], Loss: 0.0126\n",
      "Epoch [100/10], Step [10/40], Loss: 0.0128\n",
      "Epoch [100/10], Step [20/40], Loss: 0.0128\n",
      "Epoch [100/10], Step [30/40], Loss: 0.0127\n",
      "Epoch [100/10], Step [40/40], Loss: 0.0128\n",
      "Finish training\n",
      "Accuracy on training dataset: 1.0\n",
      "Accuracy on testing dataset: 1.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "binary_classification_model = ClassificationModel(10,3,1).to(device)\n",
    "binary_train_dataset = CheckDataset(data_path = \"./data/binary_input2.npy\",\n",
    "                                                labels_path= \"./data/binary_labels2.npy\")\n",
    "binary_test_dataset = CheckDataset(data_path = \"./data/binary_test2.npy\",\n",
    "                                               labels_path= \"./data/binary_labels_test2.npy\")\n",
    "\n",
    "binary_classification_train_loader = DataLoader(dataset=binary_train_dataset, batch_size=10, shuffle=True)\n",
    "binary_classification_test_loader = DataLoader(dataset=binary_test_dataset, batch_size=10, shuffle=False)\n",
    "optimizer = torch.optim.SGD(binary_classification_model.parameters(), lr=0.01)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "n_total_steps = len(binary_classification_train_loader)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, (inputs, labels) in enumerate(binary_classification_train_loader):\n",
    "\n",
    "        # forward pass and loss\n",
    "        outputs = binary_classification_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # compute gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Finish training\")\n",
    "\n",
    "print(\"Accuracy on training dataset: {}\".format(get_acc(binary_classification_model, binary_train_dataset)))\n",
    "print(\"Accuracy on testing dataset: {}\".format(get_acc(binary_classification_model, binary_test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal training multiclass classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\test\\AppData\\Local\\Temp\\ipykernel_19112\\319440667.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.ac(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/40], Loss: 1.3979\n",
      "Epoch [1/10], Step [20/40], Loss: 1.3784\n",
      "Epoch [1/10], Step [30/40], Loss: 1.3936\n",
      "Epoch [1/10], Step [40/40], Loss: 1.3890\n",
      "Epoch [2/10], Step [10/40], Loss: 1.3735\n",
      "Epoch [2/10], Step [20/40], Loss: 1.3930\n",
      "Epoch [2/10], Step [30/40], Loss: 1.3972\n",
      "Epoch [2/10], Step [40/40], Loss: 1.4001\n",
      "Epoch [3/10], Step [10/40], Loss: 1.3732\n",
      "Epoch [3/10], Step [20/40], Loss: 1.3842\n",
      "Epoch [3/10], Step [30/40], Loss: 1.3967\n",
      "Epoch [3/10], Step [40/40], Loss: 1.3881\n",
      "Epoch [4/10], Step [10/40], Loss: 1.3952\n",
      "Epoch [4/10], Step [20/40], Loss: 1.3784\n",
      "Epoch [4/10], Step [30/40], Loss: 1.3848\n",
      "Epoch [4/10], Step [40/40], Loss: 1.3756\n",
      "Epoch [5/10], Step [10/40], Loss: 1.4074\n",
      "Epoch [5/10], Step [20/40], Loss: 1.3764\n",
      "Epoch [5/10], Step [30/40], Loss: 1.3613\n",
      "Epoch [5/10], Step [40/40], Loss: 1.3777\n",
      "Epoch [6/10], Step [10/40], Loss: 1.3554\n",
      "Epoch [6/10], Step [20/40], Loss: 1.3781\n",
      "Epoch [6/10], Step [30/40], Loss: 1.3878\n",
      "Epoch [6/10], Step [40/40], Loss: 1.3776\n",
      "Epoch [7/10], Step [10/40], Loss: 1.3772\n",
      "Epoch [7/10], Step [20/40], Loss: 1.3644\n",
      "Epoch [7/10], Step [30/40], Loss: 1.3745\n",
      "Epoch [7/10], Step [40/40], Loss: 1.3719\n",
      "Epoch [8/10], Step [10/40], Loss: 1.3407\n",
      "Epoch [8/10], Step [20/40], Loss: 1.3256\n",
      "Epoch [8/10], Step [30/40], Loss: 1.3344\n",
      "Epoch [8/10], Step [40/40], Loss: 1.3444\n",
      "Epoch [9/10], Step [10/40], Loss: 1.3385\n",
      "Epoch [9/10], Step [20/40], Loss: 1.2848\n",
      "Epoch [9/10], Step [30/40], Loss: 1.3384\n",
      "Epoch [9/10], Step [40/40], Loss: 1.3281\n",
      "Epoch [10/10], Step [10/40], Loss: 1.3510\n",
      "Epoch [10/10], Step [20/40], Loss: 1.3266\n",
      "Epoch [10/10], Step [30/40], Loss: 1.2759\n",
      "Epoch [10/10], Step [40/40], Loss: 1.3243\n",
      "Epoch [11/10], Step [10/40], Loss: 1.3185\n",
      "Epoch [11/10], Step [20/40], Loss: 1.2708\n",
      "Epoch [11/10], Step [30/40], Loss: 1.2649\n",
      "Epoch [11/10], Step [40/40], Loss: 1.2921\n",
      "Epoch [12/10], Step [10/40], Loss: 1.2670\n",
      "Epoch [12/10], Step [20/40], Loss: 1.3176\n",
      "Epoch [12/10], Step [30/40], Loss: 1.2619\n",
      "Epoch [12/10], Step [40/40], Loss: 1.2599\n",
      "Epoch [13/10], Step [10/40], Loss: 1.2882\n",
      "Epoch [13/10], Step [20/40], Loss: 1.2547\n",
      "Epoch [13/10], Step [30/40], Loss: 1.2222\n",
      "Epoch [13/10], Step [40/40], Loss: 1.1920\n",
      "Epoch [14/10], Step [10/40], Loss: 1.1542\n",
      "Epoch [14/10], Step [20/40], Loss: 1.1940\n",
      "Epoch [14/10], Step [30/40], Loss: 1.2434\n",
      "Epoch [14/10], Step [40/40], Loss: 1.2051\n",
      "Epoch [15/10], Step [10/40], Loss: 1.2454\n",
      "Epoch [15/10], Step [20/40], Loss: 1.1786\n",
      "Epoch [15/10], Step [30/40], Loss: 1.2820\n",
      "Epoch [15/10], Step [40/40], Loss: 1.1737\n",
      "Epoch [16/10], Step [10/40], Loss: 1.1472\n",
      "Epoch [16/10], Step [20/40], Loss: 1.1117\n",
      "Epoch [16/10], Step [30/40], Loss: 1.1697\n",
      "Epoch [16/10], Step [40/40], Loss: 1.1366\n",
      "Epoch [17/10], Step [10/40], Loss: 1.1894\n",
      "Epoch [17/10], Step [20/40], Loss: 1.2179\n",
      "Epoch [17/10], Step [30/40], Loss: 1.1588\n",
      "Epoch [17/10], Step [40/40], Loss: 1.1041\n",
      "Epoch [18/10], Step [10/40], Loss: 1.0801\n",
      "Epoch [18/10], Step [20/40], Loss: 1.1441\n",
      "Epoch [18/10], Step [30/40], Loss: 1.1111\n",
      "Epoch [18/10], Step [40/40], Loss: 1.1120\n",
      "Epoch [19/10], Step [10/40], Loss: 1.1494\n",
      "Epoch [19/10], Step [20/40], Loss: 1.1142\n",
      "Epoch [19/10], Step [30/40], Loss: 1.1392\n",
      "Epoch [19/10], Step [40/40], Loss: 1.0864\n",
      "Epoch [20/10], Step [10/40], Loss: 1.1166\n",
      "Epoch [20/10], Step [20/40], Loss: 1.1457\n",
      "Epoch [20/10], Step [30/40], Loss: 1.0174\n",
      "Epoch [20/10], Step [40/40], Loss: 1.1387\n",
      "Epoch [21/10], Step [10/40], Loss: 1.1276\n",
      "Epoch [21/10], Step [20/40], Loss: 1.0847\n",
      "Epoch [21/10], Step [30/40], Loss: 1.0387\n",
      "Epoch [21/10], Step [40/40], Loss: 1.0541\n",
      "Epoch [22/10], Step [10/40], Loss: 1.0385\n",
      "Epoch [22/10], Step [20/40], Loss: 1.1189\n",
      "Epoch [22/10], Step [30/40], Loss: 1.0518\n",
      "Epoch [22/10], Step [40/40], Loss: 1.0543\n",
      "Epoch [23/10], Step [10/40], Loss: 1.0363\n",
      "Epoch [23/10], Step [20/40], Loss: 1.0394\n",
      "Epoch [23/10], Step [30/40], Loss: 1.0307\n",
      "Epoch [23/10], Step [40/40], Loss: 1.0382\n",
      "Epoch [24/10], Step [10/40], Loss: 1.0383\n",
      "Epoch [24/10], Step [20/40], Loss: 1.0889\n",
      "Epoch [24/10], Step [30/40], Loss: 1.0465\n",
      "Epoch [24/10], Step [40/40], Loss: 1.1135\n",
      "Epoch [25/10], Step [10/40], Loss: 1.0057\n",
      "Epoch [25/10], Step [20/40], Loss: 1.0791\n",
      "Epoch [25/10], Step [30/40], Loss: 0.9783\n",
      "Epoch [25/10], Step [40/40], Loss: 1.0271\n",
      "Epoch [26/10], Step [10/40], Loss: 1.0235\n",
      "Epoch [26/10], Step [20/40], Loss: 1.0348\n",
      "Epoch [26/10], Step [30/40], Loss: 1.0082\n",
      "Epoch [26/10], Step [40/40], Loss: 1.0069\n",
      "Epoch [27/10], Step [10/40], Loss: 1.0137\n",
      "Epoch [27/10], Step [20/40], Loss: 1.0117\n",
      "Epoch [27/10], Step [30/40], Loss: 0.9962\n",
      "Epoch [27/10], Step [40/40], Loss: 1.0287\n",
      "Epoch [28/10], Step [10/40], Loss: 0.9283\n",
      "Epoch [28/10], Step [20/40], Loss: 0.9750\n",
      "Epoch [28/10], Step [30/40], Loss: 1.0092\n",
      "Epoch [28/10], Step [40/40], Loss: 0.9527\n",
      "Epoch [29/10], Step [10/40], Loss: 0.9965\n",
      "Epoch [29/10], Step [20/40], Loss: 0.9697\n",
      "Epoch [29/10], Step [30/40], Loss: 0.9856\n",
      "Epoch [29/10], Step [40/40], Loss: 0.9771\n",
      "Epoch [30/10], Step [10/40], Loss: 1.0157\n",
      "Epoch [30/10], Step [20/40], Loss: 0.9472\n",
      "Epoch [30/10], Step [30/40], Loss: 0.9999\n",
      "Epoch [30/10], Step [40/40], Loss: 0.9422\n",
      "Epoch [31/10], Step [10/40], Loss: 0.9354\n",
      "Epoch [31/10], Step [20/40], Loss: 0.9578\n",
      "Epoch [31/10], Step [30/40], Loss: 0.9543\n",
      "Epoch [31/10], Step [40/40], Loss: 0.9635\n",
      "Epoch [32/10], Step [10/40], Loss: 0.9407\n",
      "Epoch [32/10], Step [20/40], Loss: 0.9777\n",
      "Epoch [32/10], Step [30/40], Loss: 0.9530\n",
      "Epoch [32/10], Step [40/40], Loss: 0.9591\n",
      "Epoch [33/10], Step [10/40], Loss: 0.9832\n",
      "Epoch [33/10], Step [20/40], Loss: 0.9564\n",
      "Epoch [33/10], Step [30/40], Loss: 0.9400\n",
      "Epoch [33/10], Step [40/40], Loss: 0.8974\n",
      "Epoch [34/10], Step [10/40], Loss: 0.9156\n",
      "Epoch [34/10], Step [20/40], Loss: 0.9464\n",
      "Epoch [34/10], Step [30/40], Loss: 0.9578\n",
      "Epoch [34/10], Step [40/40], Loss: 0.9273\n",
      "Epoch [35/10], Step [10/40], Loss: 0.9101\n",
      "Epoch [35/10], Step [20/40], Loss: 0.9551\n",
      "Epoch [35/10], Step [30/40], Loss: 0.9214\n",
      "Epoch [35/10], Step [40/40], Loss: 0.9032\n",
      "Epoch [36/10], Step [10/40], Loss: 0.8867\n",
      "Epoch [36/10], Step [20/40], Loss: 0.9219\n",
      "Epoch [36/10], Step [30/40], Loss: 0.9380\n",
      "Epoch [36/10], Step [40/40], Loss: 0.9215\n",
      "Epoch [37/10], Step [10/40], Loss: 0.9079\n",
      "Epoch [37/10], Step [20/40], Loss: 0.9248\n",
      "Epoch [37/10], Step [30/40], Loss: 0.9170\n",
      "Epoch [37/10], Step [40/40], Loss: 0.9160\n",
      "Epoch [38/10], Step [10/40], Loss: 0.9189\n",
      "Epoch [38/10], Step [20/40], Loss: 0.8913\n",
      "Epoch [38/10], Step [30/40], Loss: 0.9101\n",
      "Epoch [38/10], Step [40/40], Loss: 0.9131\n",
      "Epoch [39/10], Step [10/40], Loss: 0.8964\n",
      "Epoch [39/10], Step [20/40], Loss: 0.8936\n",
      "Epoch [39/10], Step [30/40], Loss: 0.9332\n",
      "Epoch [39/10], Step [40/40], Loss: 0.9118\n",
      "Epoch [40/10], Step [10/40], Loss: 0.9048\n",
      "Epoch [40/10], Step [20/40], Loss: 0.9361\n",
      "Epoch [40/10], Step [30/40], Loss: 0.9007\n",
      "Epoch [40/10], Step [40/40], Loss: 0.9082\n",
      "Epoch [41/10], Step [10/40], Loss: 0.8845\n",
      "Epoch [41/10], Step [20/40], Loss: 0.8732\n",
      "Epoch [41/10], Step [30/40], Loss: 0.8712\n",
      "Epoch [41/10], Step [40/40], Loss: 0.8593\n",
      "Epoch [42/10], Step [10/40], Loss: 0.8582\n",
      "Epoch [42/10], Step [20/40], Loss: 0.8717\n",
      "Epoch [42/10], Step [30/40], Loss: 0.9159\n",
      "Epoch [42/10], Step [40/40], Loss: 0.8674\n",
      "Epoch [43/10], Step [10/40], Loss: 0.8711\n",
      "Epoch [43/10], Step [20/40], Loss: 0.8614\n",
      "Epoch [43/10], Step [30/40], Loss: 0.8657\n",
      "Epoch [43/10], Step [40/40], Loss: 0.8843\n",
      "Epoch [44/10], Step [10/40], Loss: 0.8669\n",
      "Epoch [44/10], Step [20/40], Loss: 0.8870\n",
      "Epoch [44/10], Step [30/40], Loss: 0.8710\n",
      "Epoch [44/10], Step [40/40], Loss: 0.8774\n",
      "Epoch [45/10], Step [10/40], Loss: 0.8704\n",
      "Epoch [45/10], Step [20/40], Loss: 0.8878\n",
      "Epoch [45/10], Step [30/40], Loss: 0.8539\n",
      "Epoch [45/10], Step [40/40], Loss: 0.8547\n",
      "Epoch [46/10], Step [10/40], Loss: 0.8654\n",
      "Epoch [46/10], Step [20/40], Loss: 0.8670\n",
      "Epoch [46/10], Step [30/40], Loss: 0.8590\n",
      "Epoch [46/10], Step [40/40], Loss: 0.8383\n",
      "Epoch [47/10], Step [10/40], Loss: 0.8567\n",
      "Epoch [47/10], Step [20/40], Loss: 0.8745\n",
      "Epoch [47/10], Step [30/40], Loss: 0.8473\n",
      "Epoch [47/10], Step [40/40], Loss: 0.8527\n",
      "Epoch [48/10], Step [10/40], Loss: 0.8534\n",
      "Epoch [48/10], Step [20/40], Loss: 0.8567\n",
      "Epoch [48/10], Step [30/40], Loss: 0.8740\n",
      "Epoch [48/10], Step [40/40], Loss: 0.8811\n",
      "Epoch [49/10], Step [10/40], Loss: 0.8634\n",
      "Epoch [49/10], Step [20/40], Loss: 0.8493\n",
      "Epoch [49/10], Step [30/40], Loss: 0.8520\n",
      "Epoch [49/10], Step [40/40], Loss: 0.8556\n",
      "Epoch [50/10], Step [10/40], Loss: 0.8383\n",
      "Epoch [50/10], Step [20/40], Loss: 0.8499\n",
      "Epoch [50/10], Step [30/40], Loss: 0.8521\n",
      "Epoch [50/10], Step [40/40], Loss: 0.8533\n",
      "Epoch [51/10], Step [10/40], Loss: 0.8312\n",
      "Epoch [51/10], Step [20/40], Loss: 0.8462\n",
      "Epoch [51/10], Step [30/40], Loss: 0.8536\n",
      "Epoch [51/10], Step [40/40], Loss: 0.8447\n",
      "Epoch [52/10], Step [10/40], Loss: 0.8671\n",
      "Epoch [52/10], Step [20/40], Loss: 0.8416\n",
      "Epoch [52/10], Step [30/40], Loss: 0.8674\n",
      "Epoch [52/10], Step [40/40], Loss: 0.8376\n",
      "Epoch [53/10], Step [10/40], Loss: 0.8515\n",
      "Epoch [53/10], Step [20/40], Loss: 0.8516\n",
      "Epoch [53/10], Step [30/40], Loss: 0.8401\n",
      "Epoch [53/10], Step [40/40], Loss: 0.8256\n",
      "Epoch [54/10], Step [10/40], Loss: 0.8507\n",
      "Epoch [54/10], Step [20/40], Loss: 0.8278\n",
      "Epoch [54/10], Step [30/40], Loss: 0.8391\n",
      "Epoch [54/10], Step [40/40], Loss: 0.8394\n",
      "Epoch [55/10], Step [10/40], Loss: 0.8534\n",
      "Epoch [55/10], Step [20/40], Loss: 0.8407\n",
      "Epoch [55/10], Step [30/40], Loss: 0.8333\n",
      "Epoch [55/10], Step [40/40], Loss: 0.8308\n",
      "Epoch [56/10], Step [10/40], Loss: 0.8403\n",
      "Epoch [56/10], Step [20/40], Loss: 0.8238\n",
      "Epoch [56/10], Step [30/40], Loss: 0.8599\n",
      "Epoch [56/10], Step [40/40], Loss: 0.8359\n",
      "Epoch [57/10], Step [10/40], Loss: 0.8352\n",
      "Epoch [57/10], Step [20/40], Loss: 0.8428\n",
      "Epoch [57/10], Step [30/40], Loss: 0.8301\n",
      "Epoch [57/10], Step [40/40], Loss: 0.8358\n",
      "Epoch [58/10], Step [10/40], Loss: 0.8258\n",
      "Epoch [58/10], Step [20/40], Loss: 0.8320\n",
      "Epoch [58/10], Step [30/40], Loss: 0.8216\n",
      "Epoch [58/10], Step [40/40], Loss: 0.8499\n",
      "Epoch [59/10], Step [10/40], Loss: 0.8265\n",
      "Epoch [59/10], Step [20/40], Loss: 0.8292\n",
      "Epoch [59/10], Step [30/40], Loss: 0.8289\n",
      "Epoch [59/10], Step [40/40], Loss: 0.8404\n",
      "Epoch [60/10], Step [10/40], Loss: 0.8434\n",
      "Epoch [60/10], Step [20/40], Loss: 0.8286\n",
      "Epoch [60/10], Step [30/40], Loss: 0.8072\n",
      "Epoch [60/10], Step [40/40], Loss: 0.8321\n",
      "Epoch [61/10], Step [10/40], Loss: 0.8393\n",
      "Epoch [61/10], Step [20/40], Loss: 0.8224\n",
      "Epoch [61/10], Step [30/40], Loss: 0.8111\n",
      "Epoch [61/10], Step [40/40], Loss: 0.8297\n",
      "Epoch [62/10], Step [10/40], Loss: 0.8288\n",
      "Epoch [62/10], Step [20/40], Loss: 0.8146\n",
      "Epoch [62/10], Step [30/40], Loss: 0.8363\n",
      "Epoch [62/10], Step [40/40], Loss: 0.8213\n",
      "Epoch [63/10], Step [10/40], Loss: 0.8262\n",
      "Epoch [63/10], Step [20/40], Loss: 0.8272\n",
      "Epoch [63/10], Step [30/40], Loss: 0.8185\n",
      "Epoch [63/10], Step [40/40], Loss: 0.8261\n",
      "Epoch [64/10], Step [10/40], Loss: 0.8073\n",
      "Epoch [64/10], Step [20/40], Loss: 0.8350\n",
      "Epoch [64/10], Step [30/40], Loss: 0.8285\n",
      "Epoch [64/10], Step [40/40], Loss: 0.8160\n",
      "Epoch [65/10], Step [10/40], Loss: 0.8128\n",
      "Epoch [65/10], Step [20/40], Loss: 0.8178\n",
      "Epoch [65/10], Step [30/40], Loss: 0.8147\n",
      "Epoch [65/10], Step [40/40], Loss: 0.8230\n",
      "Epoch [66/10], Step [10/40], Loss: 0.8254\n",
      "Epoch [66/10], Step [20/40], Loss: 0.8161\n",
      "Epoch [66/10], Step [30/40], Loss: 0.8158\n",
      "Epoch [66/10], Step [40/40], Loss: 0.8041\n",
      "Epoch [67/10], Step [10/40], Loss: 0.8206\n",
      "Epoch [67/10], Step [20/40], Loss: 0.8239\n",
      "Epoch [67/10], Step [30/40], Loss: 0.8085\n",
      "Epoch [67/10], Step [40/40], Loss: 0.8194\n",
      "Epoch [68/10], Step [10/40], Loss: 0.8039\n",
      "Epoch [68/10], Step [20/40], Loss: 0.8151\n",
      "Epoch [68/10], Step [30/40], Loss: 0.8110\n",
      "Epoch [68/10], Step [40/40], Loss: 0.8130\n",
      "Epoch [69/10], Step [10/40], Loss: 0.8255\n",
      "Epoch [69/10], Step [20/40], Loss: 0.8138\n",
      "Epoch [69/10], Step [30/40], Loss: 0.8265\n",
      "Epoch [69/10], Step [40/40], Loss: 0.8029\n",
      "Epoch [70/10], Step [10/40], Loss: 0.8163\n",
      "Epoch [70/10], Step [20/40], Loss: 0.8212\n",
      "Epoch [70/10], Step [30/40], Loss: 0.8174\n",
      "Epoch [70/10], Step [40/40], Loss: 0.8153\n",
      "Epoch [71/10], Step [10/40], Loss: 0.8183\n",
      "Epoch [71/10], Step [20/40], Loss: 0.8063\n",
      "Epoch [71/10], Step [30/40], Loss: 0.8143\n",
      "Epoch [71/10], Step [40/40], Loss: 0.8042\n",
      "Epoch [72/10], Step [10/40], Loss: 0.8099\n",
      "Epoch [72/10], Step [20/40], Loss: 0.7972\n",
      "Epoch [72/10], Step [30/40], Loss: 0.8256\n",
      "Epoch [72/10], Step [40/40], Loss: 0.8120\n",
      "Epoch [73/10], Step [10/40], Loss: 0.8085\n",
      "Epoch [73/10], Step [20/40], Loss: 0.8099\n",
      "Epoch [73/10], Step [30/40], Loss: 0.8085\n",
      "Epoch [73/10], Step [40/40], Loss: 0.8062\n",
      "Epoch [74/10], Step [10/40], Loss: 0.8043\n",
      "Epoch [74/10], Step [20/40], Loss: 0.8116\n",
      "Epoch [74/10], Step [30/40], Loss: 0.8086\n",
      "Epoch [74/10], Step [40/40], Loss: 0.8053\n",
      "Epoch [75/10], Step [10/40], Loss: 0.8101\n",
      "Epoch [75/10], Step [20/40], Loss: 0.8036\n",
      "Epoch [75/10], Step [30/40], Loss: 0.8110\n",
      "Epoch [75/10], Step [40/40], Loss: 0.8061\n",
      "Epoch [76/10], Step [10/40], Loss: 0.7990\n",
      "Epoch [76/10], Step [20/40], Loss: 0.7987\n",
      "Epoch [76/10], Step [30/40], Loss: 0.8083\n",
      "Epoch [76/10], Step [40/40], Loss: 0.8140\n",
      "Epoch [77/10], Step [10/40], Loss: 0.7997\n",
      "Epoch [77/10], Step [20/40], Loss: 0.8027\n",
      "Epoch [77/10], Step [30/40], Loss: 0.8107\n",
      "Epoch [77/10], Step [40/40], Loss: 0.8022\n",
      "Epoch [78/10], Step [10/40], Loss: 0.7984\n",
      "Epoch [78/10], Step [20/40], Loss: 0.8004\n",
      "Epoch [78/10], Step [30/40], Loss: 0.7992\n",
      "Epoch [78/10], Step [40/40], Loss: 0.8042\n",
      "Epoch [79/10], Step [10/40], Loss: 0.8044\n",
      "Epoch [79/10], Step [20/40], Loss: 0.7980\n",
      "Epoch [79/10], Step [30/40], Loss: 0.8095\n",
      "Epoch [79/10], Step [40/40], Loss: 0.8084\n",
      "Epoch [80/10], Step [10/40], Loss: 0.7968\n",
      "Epoch [80/10], Step [20/40], Loss: 0.8074\n",
      "Epoch [80/10], Step [30/40], Loss: 0.8009\n",
      "Epoch [80/10], Step [40/40], Loss: 0.8075\n",
      "Epoch [81/10], Step [10/40], Loss: 0.8059\n",
      "Epoch [81/10], Step [20/40], Loss: 0.8059\n",
      "Epoch [81/10], Step [30/40], Loss: 0.8017\n",
      "Epoch [81/10], Step [40/40], Loss: 0.8002\n",
      "Epoch [82/10], Step [10/40], Loss: 0.8014\n",
      "Epoch [82/10], Step [20/40], Loss: 0.7966\n",
      "Epoch [82/10], Step [30/40], Loss: 0.8017\n",
      "Epoch [82/10], Step [40/40], Loss: 0.7928\n",
      "Epoch [83/10], Step [10/40], Loss: 0.8014\n",
      "Epoch [83/10], Step [20/40], Loss: 0.8016\n",
      "Epoch [83/10], Step [30/40], Loss: 0.8020\n",
      "Epoch [83/10], Step [40/40], Loss: 0.8061\n",
      "Epoch [84/10], Step [10/40], Loss: 0.8095\n",
      "Epoch [84/10], Step [20/40], Loss: 0.7940\n",
      "Epoch [84/10], Step [30/40], Loss: 0.8109\n",
      "Epoch [84/10], Step [40/40], Loss: 0.7955\n",
      "Epoch [85/10], Step [10/40], Loss: 0.7983\n",
      "Epoch [85/10], Step [20/40], Loss: 0.7936\n",
      "Epoch [85/10], Step [30/40], Loss: 0.7984\n",
      "Epoch [85/10], Step [40/40], Loss: 0.7968\n",
      "Epoch [86/10], Step [10/40], Loss: 0.8031\n",
      "Epoch [86/10], Step [20/40], Loss: 0.7987\n",
      "Epoch [86/10], Step [30/40], Loss: 0.8002\n",
      "Epoch [86/10], Step [40/40], Loss: 0.7999\n",
      "Epoch [87/10], Step [10/40], Loss: 0.7957\n",
      "Epoch [87/10], Step [20/40], Loss: 0.7928\n",
      "Epoch [87/10], Step [30/40], Loss: 0.8000\n",
      "Epoch [87/10], Step [40/40], Loss: 0.7946\n",
      "Epoch [88/10], Step [10/40], Loss: 0.7931\n",
      "Epoch [88/10], Step [20/40], Loss: 0.8045\n",
      "Epoch [88/10], Step [30/40], Loss: 0.7912\n",
      "Epoch [88/10], Step [40/40], Loss: 0.7971\n",
      "Epoch [89/10], Step [10/40], Loss: 0.7974\n",
      "Epoch [89/10], Step [20/40], Loss: 0.7971\n",
      "Epoch [89/10], Step [30/40], Loss: 0.7915\n",
      "Epoch [89/10], Step [40/40], Loss: 0.7922\n",
      "Epoch [90/10], Step [10/40], Loss: 0.7941\n",
      "Epoch [90/10], Step [20/40], Loss: 0.8007\n",
      "Epoch [90/10], Step [30/40], Loss: 0.7959\n",
      "Epoch [90/10], Step [40/40], Loss: 0.7957\n",
      "Epoch [91/10], Step [10/40], Loss: 0.7905\n",
      "Epoch [91/10], Step [20/40], Loss: 0.7986\n",
      "Epoch [91/10], Step [30/40], Loss: 0.7925\n",
      "Epoch [91/10], Step [40/40], Loss: 0.7937\n",
      "Epoch [92/10], Step [10/40], Loss: 0.7999\n",
      "Epoch [92/10], Step [20/40], Loss: 0.7843\n",
      "Epoch [92/10], Step [30/40], Loss: 0.7905\n",
      "Epoch [92/10], Step [40/40], Loss: 0.7971\n",
      "Epoch [93/10], Step [10/40], Loss: 0.7909\n",
      "Epoch [93/10], Step [20/40], Loss: 0.7808\n",
      "Epoch [93/10], Step [30/40], Loss: 0.7911\n",
      "Epoch [93/10], Step [40/40], Loss: 0.7920\n",
      "Epoch [94/10], Step [10/40], Loss: 0.7884\n",
      "Epoch [94/10], Step [20/40], Loss: 0.7913\n",
      "Epoch [94/10], Step [30/40], Loss: 0.7925\n",
      "Epoch [94/10], Step [40/40], Loss: 0.7946\n",
      "Epoch [95/10], Step [10/40], Loss: 0.7940\n",
      "Epoch [95/10], Step [20/40], Loss: 0.7944\n",
      "Epoch [95/10], Step [30/40], Loss: 0.7892\n",
      "Epoch [95/10], Step [40/40], Loss: 0.7833\n",
      "Epoch [96/10], Step [10/40], Loss: 0.7967\n",
      "Epoch [96/10], Step [20/40], Loss: 0.7836\n",
      "Epoch [96/10], Step [30/40], Loss: 0.7887\n",
      "Epoch [96/10], Step [40/40], Loss: 0.7923\n",
      "Epoch [97/10], Step [10/40], Loss: 0.7969\n",
      "Epoch [97/10], Step [20/40], Loss: 0.7957\n",
      "Epoch [97/10], Step [30/40], Loss: 0.7878\n",
      "Epoch [97/10], Step [40/40], Loss: 0.7849\n",
      "Epoch [98/10], Step [10/40], Loss: 0.7945\n",
      "Epoch [98/10], Step [20/40], Loss: 0.7846\n",
      "Epoch [98/10], Step [30/40], Loss: 0.7838\n",
      "Epoch [98/10], Step [40/40], Loss: 0.7934\n",
      "Epoch [99/10], Step [10/40], Loss: 0.7929\n",
      "Epoch [99/10], Step [20/40], Loss: 0.7936\n",
      "Epoch [99/10], Step [30/40], Loss: 0.7888\n",
      "Epoch [99/10], Step [40/40], Loss: 0.7885\n",
      "Epoch [100/10], Step [10/40], Loss: 0.7909\n",
      "Epoch [100/10], Step [20/40], Loss: 0.7916\n",
      "Epoch [100/10], Step [30/40], Loss: 0.7875\n",
      "Epoch [100/10], Step [40/40], Loss: 0.7845\n",
      "Finish training\n",
      "Accuracy on training dataset: 1.0\n",
      "Accuracy on testing dataset: 1.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classification_model = ClassificationModel(10,3,4,multiclass=True).to(device)\n",
    "multiclass_train_dataset = CheckDataset(data_path = \"./data/binary_input4.npy\",\n",
    "                                          labels_path= \"./data/binary_labels4.npy\",\n",
    "                                          multiclass= True)\n",
    "multiclass_test_dataset = CheckDataset(data_path = \"./data/binary_test4.npy\",\n",
    "                                         labels_path= \"./data/binary_labels_test4.npy\",\n",
    "                                         multiclass= True)\n",
    "\n",
    "train_loader = DataLoader(dataset=multiclass_train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(dataset=multiclass_test_dataset, batch_size=10, shuffle=False)\n",
    "optimizer = torch.optim.SGD(classification_model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # forward pass and loss\n",
    "        outputs = classification_model(inputs)\n",
    "\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # compute gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Finish training\")\n",
    "\n",
    "print(\"Accuracy on training dataset: {}\".format(get_acc(classification_model, binary_train_dataset, multiclass=True)))\n",
    "print(\"Accuracy on testing dataset: {}\".format(get_acc(classification_model, multiclass_test_dataset, multiclass=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5a68525395ef494d8747156e7e9d0075f9a1eafcfe045277097ee7133a38f7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
